/*
 * Adapted from https://github.com/NVIDIA/FasterTransformer/blob/release/v5.3_tag/src/fastertransformer/kernels/decoder_masked_multihead_attention/decoder_masked_multihead_attention_template.hpp
 * Copyright (c) 2023, The vLLM team.
 * Copyright (c) 2020-2023, NVIDIA CORPORATION.  All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#include <torch/extension.h>
#include <ATen/cuda/CUDAContext.h>

#include "attention_dtypes.h"
#include "attention_utils.cuh"

#include <algorithm>

#define WARP_SIZE 32
#define MAX(a, b) ((a) > (b) ? (a) : (b))
#define MIN(a, b) ((a) < (b) ? (a) : (b))

namespace vllm {

// Utility function for attention softmax.
template<int NUM_WARPS>
inline __device__ float block_sum(float* red_smem, float sum) {
  // Decompose the thread index into warp / lane.
  int warp = threadIdx.x / WARP_SIZE;
  int lane = threadIdx.x % WARP_SIZE;

  // Compute the sum per warp.
#pragma unroll
  for (int mask = WARP_SIZE / 2; mask >= 1; mask /= 2) {
    sum += __shfl_xor_sync(uint32_t(-1), sum, mask);
  }

  // Warp leaders store the data to shared memory.
  if (lane == 0) {
    red_smem[warp] = sum;
  }

  // Make sure the data is in shared memory.
  __syncthreads();

  // The warps compute the final sums.
  if (lane < NUM_WARPS) {
    sum = red_smem[lane];
  }

  // Parallel reduction inside the warp.
#pragma unroll
  for (int mask = NUM_WARPS / 2; mask >= 1; mask /= 2) {
    sum += __shfl_xor_sync(uint32_t(-1), sum, mask);
  }

  // Broadcast to other threads.
  return __shfl_sync(uint32_t(-1), sum, 0);
}


/*
dim3 grid(num_heads, num_seqs);
dim3 block(NUM_THREADS);
*/
template<typename scalar_t, int HEAD_SIZE, int BLOCK_SIZE, int NUM_THREADS>
__global__ void xl_single_query_cached_kv_attention_kernel(
  scalar_t* __restrict__ out,             // [num_seqs, num_heads, head_size]
  const scalar_t* __restrict__ q,         // [num_seqs, num_heads, head_size]
  const scalar_t* __restrict__ k_cache,   // [num_blocks, num_heads, head_size/x, block_size, x]
  const scalar_t* __restrict__ v_cache,   // [num_blocks, num_heads, head_size, block_size]
  const scalar_t* __restrict__ pos_bias_u, // [num_seqs, num_heads, head_size]
  const scalar_t* __restrict__ matrix_bd, // [num_seqs, num_heads, num_tokens]
  const float scale,
  const int* __restrict__ block_tables,   // [num_seqs, max_num_blocks_per_seq]
  const int* __restrict__ context_lens,   // [num_seqs]
  const int max_num_blocks_per_seq,
  const int q_stride) 
{
  constexpr int THREAD_GROUP_SIZE = MAX(WARP_SIZE / BLOCK_SIZE, 1); // assume BLOCK_SIZE is 16, THREAD_GROUP_SIZE=2
  constexpr int NUM_TOKENS_PER_THREAD_GROUP = (BLOCK_SIZE + WARP_SIZE - 1) / WARP_SIZE; // (16 + 31) / 32 =1
  constexpr int NUM_WARPS = NUM_THREADS / WARP_SIZE; // 128 / 32 = 4
  const int thread_idx = threadIdx.x;  // 0, 1, 2 , ... , 127
  const int warp_idx = thread_idx / WARP_SIZE; // 0, 0 , ... , 1, 1, ..., 2, 2, ... , 3, 3 ...
  const int lane = thread_idx % WARP_SIZE;// 0, 1 , 2 ..., 31,  0, 1 , 2 ..., 31,  0, 1 , 2 ..., 31,  0, 1 , 2 ..., 31  
  const int head_idx = blockIdx.x;  // 0, 1, 2, ..., 15
  const int num_heads = gridDim.x; // number of heads, assume dim is 2048, head size is 128, then num_heads is 16
  const int seq_idx = blockIdx.y; // 0, 1, 2, ..., num_seqs - 1
  // A vector type to store a part of a key or a query.
  // The vector size is configured in such a way that the threads in a thread group
  // fetch or compute 16 bytes at a time.
  // For example, if the size of a thread group is 4 and the data type is half,
  // then the vector size is 16 / (4 * sizeof(half)) == 2.
  // 16 / (2 * 2) = 4
  constexpr int VEC_SIZE = MAX(16 / (THREAD_GROUP_SIZE * sizeof(scalar_t)), 1);
  using K_vec = typename Vec<scalar_t, VEC_SIZE>::Type;
  using Q_vec = typename Vec<scalar_t, VEC_SIZE>::Type;
  constexpr int NUM_ELEMS_PER_THREAD = HEAD_SIZE / THREAD_GROUP_SIZE; // 128/2 = 64
  constexpr int NUM_VECS_PER_THREAD = NUM_ELEMS_PER_THREAD / VEC_SIZE; // 64/4 = 16 
  const int thread_group_idx = thread_idx / THREAD_GROUP_SIZE; // 0, 0, 1, 1, ..., 63, 63 -- two elem keep a group
  const int thread_group_offset = thread_idx % THREAD_GROUP_SIZE;// 0, 1, 0, 1, ..., 0, 1 
  // Load the query to registers.
  // Each thread in a thread group has a different part of the query.
  // For example, if the the thread group size is 4, then the first thread in the group
  // has 0, 4, 8, ... th vectors of the query, and the second thread has 1, 5, 9, ...
  // th vectors of the query, and so on.
  // NOTE(woosuk): Because q is split from a qkv tensor, it may not be contiguous.
  const scalar_t* q_ptr = q + seq_idx * q_stride + head_idx * HEAD_SIZE;
  const scalar_t* pos_bias_u_ptr = pos_bias_u + seq_idx * q_stride + head_idx * HEAD_SIZE; // same shape with q
  Q_vec q_vecs[NUM_VECS_PER_THREAD]; 
#pragma unroll
  for (int i = 0; i < NUM_VECS_PER_THREAD; i++) {
    // 0, 1, 0, 1, ..., 0, 1 +  i (0, 1, 2, ..., 15) * 2
    // 0, 1, 0, 1, ..., 0, 1  loop 0
    // 2, 3, 2, 3, ..., 2, 3 loop 1
    // ...
    // 30, 31, 30, 31, ..., 30, 31 loop 15
    const int vec_idx = thread_group_offset + i * THREAD_GROUP_SIZE;
    // 0, 4, 0, 4, ,..., 0, 4 loop 0
    // 8, 12, 8, 12, ..., 8, 12 loop 1
    // ...
    // 30*4, 31*4, ..., 30*4, 31*4 loop 15
    q_vecs[i] = *reinterpret_cast<const Q_vec*>(q_ptr + vec_idx * VEC_SIZE);
    q_vecs[i] = add(q_vecs[i], *reinterpret_cast<const Q_vec*>(pos_bias_u_ptr + vec_idx * VEC_SIZE));
    //if(threadIdx.x < 2 && blockIdx.x < 1)
    //{
    //  float f_q = convert_float(q[0]);
    //  float f_bias = convert_float(pos_bias_u[seq_idx * q_stride + head_idx * HEAD_SIZE + vec_idx * VEC_SIZE]);
    //  float f_ad = f_q + f_bias;
    //  printf("i:%d -tid:%d- f_q:%f - f_bias:%f -- f_adq:%f  ", i, threadIdx.x, f_q, f_bias, f_ad);
    //}
        
  }

  //if(threadIdx.x < 1 && blockIdx.x < 1)
  //{
  //  printf("\n");
  //}

  // Memory planning.
  extern __shared__ char shared_mem[];
  // NOTE(woosuk): We use FP32 for the softmax logits for better accuracy.
  float* logits = reinterpret_cast<float*>(shared_mem);
  // Workspace for reduction.
  __shared__ float red_smem[2 * NUM_WARPS]; //red_smem[8]

  // x == THREAD_GROUP_SIZE * VEC_SIZE
  // Each thread group fetches x elements from the key at a time.
  constexpr int x = 16 / sizeof(scalar_t); // 16/2=8
  float qk_max = -FLT_MAX;

  const int* block_table = block_tables + seq_idx * max_num_blocks_per_seq;
  const int context_len = context_lens[seq_idx];
  const int num_blocks = (context_len + BLOCK_SIZE - 1) / BLOCK_SIZE; //(context_len + BLOCK_SIZE - 1) / BLOCK_SIZE;

  // Iterate over the key blocks.
  // Each warp fetches a block of keys for each iteration.
  // Each thread group in a warp fetches a key from the block, and computes
  // dot product with the query.
  // assume we have 8 tokens, then : 
  // token 0 1 2 3 4 5 6 7
  // warp  0 1 2 3 0 1 2 3
  for (int block_idx = warp_idx; block_idx < num_blocks; block_idx += NUM_WARPS) {

    // Load a key to registers.
    // Each thread in a thread group has a different part of the key.
    // For example, if the the thread group size is 4, then the first thread in the group
    // has 0, 4, 8, ... th vectors of the key, and the second thread has 1, 5, 9, ... th
    // vectors of the key, and so on.
    for (int i = 0; i < NUM_TOKENS_PER_THREAD_GROUP; i++) {
      //0, 0, 1, 1, ..., 15, 15 warp 0
      //0, 0, 1, 1, ..., 15, 15 warp 1
      //0, 0, 1, 1, ..., 15, 15 warp 2
      //0, 0, 1, 1, ..., 15, 15 warp 3
      const int physical_block_offset = (thread_group_idx + i * WARP_SIZE) % BLOCK_SIZE; 
      //0, 0, 1, 1, ..., 15, 15 warp 0
      //16, 16, 17, 17, ..., 31, 31 warp 1
      //32, 32, 33, 33, ..., 47, 47 warp 2
      //48, 48, 49, 49, ..., 63, 63 warp 3
      const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset;

      const int block_index = (token_idx >= context_len) ? 0 : token_idx;
      const int physical_block_number = block_table[block_index]; // different warp mapping to different block

      K_vec k_vecs[NUM_VECS_PER_THREAD]; //k_vecs[16]

#pragma unroll // 32 loops
      for (int j = 0; j < NUM_VECS_PER_THREAD; j++) {
        // base adderss + 
        // 0, 0, 8, 8, ..., 15*8, 15*8  warp 0   different warp mapping to different block
        // 0, 0, 8, 8, ..., 15*8, 15*8  warp 1
        // 0, 0, 8, 8, ..., 15*8, 15*8  warp 2
        // 0, 0, 8, 8, ..., 15*8, 15*8  warp 3
        const scalar_t* k_ptr = k_cache + physical_block_number * num_heads * HEAD_SIZE * BLOCK_SIZE
                                        + head_idx * HEAD_SIZE * BLOCK_SIZE
                                        + physical_block_offset * x;
        // 0, 1, 0, 1, ..., 0,1  +  j(0, 1, ... , 31) * 2
        // 0, 1, 0, 1, ..., 0,1        loop 0
        // 2, 3, 2, 3, ..., 2,3        loop 1
        // ...
        // 30, 31, ..., 30, 31 loop 15
        const int vec_idx = thread_group_offset + j * THREAD_GROUP_SIZE;
        // (vec_idx * 4) / 8;
        // 0, 0, ... , 0 loop 0
        // 1, 1, ... , 1 loop 1
        // 2, 2, ... , 2 loop 2
        // 3, 3, ... , 3 loop 3
        // ... 
        // 15, 15, ... , 15 loop 15
        const int offset1 = (vec_idx * VEC_SIZE) / x;
        // (vec_idx * 4) % 8;
        // 0, 4, ... , 0, 4 loop 0
        // 0, 4, ... , 0, 4 loop 1
        // 0, 4, ... , 0, 4 loop 2
        // 0, 4, ... , 0, 4 loop 3
        // ...
        // 0, 4, ... , 0, 4 loop 15
        const int offset2 = (vec_idx * VEC_SIZE) % x;
        // 0, 4, 8, 12, ..., 15*8, 15*8+4 loop 0
        // 16*8, 16*8+4, ..., 31*8, 31*8+4 loop 1
        // 32*8, 32*8+4, ..., 47*8, 47*8+4 loop 2
        // ...
        // 15*16*8, 15*16*8+4, ...,        loop 15
        k_vecs[j] = *reinterpret_cast<const K_vec*>(k_ptr + offset1 * BLOCK_SIZE * x + offset2);

    //    if(threadIdx.x < 2 && blockIdx.x < 1)
    //{
    //  float f_k = convert_float(k_cache[physical_block_number * num_heads * HEAD_SIZE * BLOCK_SIZE
    //                                    + head_idx * HEAD_SIZE * BLOCK_SIZE
    //                                    + physical_block_offset * x + offset1 * BLOCK_SIZE * x + offset2]);
    //  int s_off = physical_block_number * num_heads * HEAD_SIZE * BLOCK_SIZE
    //                                    + head_idx * HEAD_SIZE * BLOCK_SIZE
    //                                    + physical_block_offset * x + offset1 * BLOCK_SIZE * x + offset2;
    //  printf("j:%d -tid:%d  -phy:%d -off:%d -k:%f  ", j, threadIdx.x, physical_block_number,  s_off, f_k);
    //}

      }

      // Compute dot product.
      // This includes a reduction across the threads in the same thread group.
      // k_cache,   // [num_blocks, num_heads, head_size/x, block_size, x]
      // we need concate by heda dim, so, k_vecs must cross block_size
      // assume block_size is 16, x is 8, so, each date fetch need offset 16*8
      float qk = Qk_dot<scalar_t, THREAD_GROUP_SIZE>::dot(q_vecs, k_vecs);
      // when token_idx >= context_len, qk = 0
      const bool mask = token_idx >= context_len;
    
      // two elem keep a group, get tid % 2 == 0, means first elem in group
      // first .. loop reduce
      if (thread_group_offset == 0) {
        // add base offset, --> head idx
        // matrix_bd [num_seqs, num_heads, num_blocks, block_size]  -- [num_seqs, num_heads, context_len]
        const int matrix_bd_seq_base = seq_idx*num_heads*context_len + head_idx*context_len;
        //if(token_idx < context_len && blockIdx.x < 1){
        //  printf("token_idx:%d -- qk:%f  ", token_idx, qk);
        //  if (threadIdx.x == 0)
        //    printf("\n");
        //}
          
        if (token_idx < context_len){
          float matrix_bd_val = convert_float(matrix_bd[matrix_bd_seq_base + token_idx]);
          //float add_val = qk+matrix_bd_val;
          //if(threadIdx.x < 32 && blockIdx.x < 1)
          //{
          //  printf("matrix_bd id:%d -- val:%f --add:%f ", matrix_bd_seq_base+token_idx, matrix_bd_val, add_val);
          //  if (threadIdx.x == 0)
          //    printf("\n");
          //}
          //if(threadIdx.x < 32 && blockIdx.x < 1)
          //{
          //  printf("nqk id:%d --scale:%f --add_val:%f -- val:%f --oqk:%f ", token_idx, scale, add_val, (scale*add_val), qk);
          //}
          qk = (scale*(qk+matrix_bd_val));
          
        }
        // Store the partial reductions to shared memory.
        // NOTE(woosuk): It is required to zero out the masked logits.
        logits[token_idx] = mask ? 0.f : qk;
        //if(threadIdx.x < 32 && blockIdx.x <= 1)
        //{
        //  printf("logits id:%d -- val:%f  ", token_idx, logits[token_idx]);
        //}
        // Update the max value.
        // cross warp qk reduce.... eg token 0, 4, 8 ... reduce
        qk_max = mask ? qk_max : fmaxf(qk_max, qk);
      }
    }
  }

  // second warp reduce
  // Perform reduction across the threads in the same warp to get the
  // max qk value for each "warp" (not across the thread block yet).
  // The 0-th thread of each thread group already has its max qk value.
  // shfl mask: 0xffffffff
#pragma unroll
  for (int mask = WARP_SIZE / 2; mask >= THREAD_GROUP_SIZE; mask /= 2) {
    qk_max = fmaxf(qk_max, __shfl_xor_sync(uint32_t(-1), qk_max, mask));
  }
  if (lane == 0) {
    red_smem[warp_idx] = qk_max;
  }
  __syncthreads();

  // third block reduce
  // TODO(woosuk): Refactor this part.
  // Get the max qk value for the sequence.
  qk_max = lane < NUM_WARPS ? red_smem[lane] : -FLT_MAX;
#pragma unroll
  for (int mask = NUM_WARPS / 2; mask >= 1; mask /= 2) {
    qk_max = fmaxf(qk_max, __shfl_xor_sync(uint32_t(-1), qk_max, mask));
  }
  // Broadcast the max qk value to all threads.
  qk_max = __shfl_sync(uint32_t(-1), qk_max, 0);

  // Get the sum of the exp values.  1. token loop reduce
  float exp_sum = 0.f;
  for (int i = thread_idx; i < context_len; i += NUM_THREADS) {
    float val = __expf(logits[i] - qk_max);
    logits[i] = val;
    exp_sum += val;
  }
  // 2. sum of block loop reduce 
  exp_sum = block_sum<NUM_WARPS>(&red_smem[NUM_WARPS], exp_sum);

  // Compute softmax.
  const float inv_sum = __fdividef(1.f, exp_sum + 1e-6f);
  for (int i = thread_idx; i < context_len; i += NUM_THREADS) {
    logits[i] *= inv_sum;
  }
  __syncthreads();

  // Each thread will fetch 16 bytes from the value cache at a time.
  // min(16/2, 16) = 8
  constexpr int V_VEC_SIZE = MIN(16 / sizeof(scalar_t), BLOCK_SIZE);
  using V_vec = typename Vec<scalar_t, V_VEC_SIZE>::Type;
  using L_vec = typename Vec<scalar_t, V_VEC_SIZE>::Type;
  using Float_L_vec = typename FloatVec<L_vec>::Type;

  // 16/8 = 2
  constexpr int NUM_V_VECS_PER_ROW = BLOCK_SIZE / V_VEC_SIZE;
  // 32/2 = 16
  constexpr int NUM_ROWS_PER_ITER = WARP_SIZE / NUM_V_VECS_PER_ROW;
  // (128+16-1)/16 = 8
  constexpr int NUM_ROWS_PER_THREAD = (HEAD_SIZE + NUM_ROWS_PER_ITER - 1) / NUM_ROWS_PER_ITER;

  // NOTE(woosuk): We use FP32 for the accumulator for better accuracy.
  float accs[NUM_ROWS_PER_THREAD];
#pragma unroll
  for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
    accs[i] = 0.f;
  }

  for (int block_idx = warp_idx; block_idx < num_blocks; block_idx += NUM_WARPS) {
    
    // (0, 1,..., 31, 0,...31, 0,...,31, 0,...,31) % 2 * 8
    // 0, 8, 0, 8, ..., 0, 8, 0, 8, ..., 0, 8, 0, 8, ..., 0, 8, 0, 8
    const int physical_block_offset = (lane % NUM_V_VECS_PER_ROW) * V_VEC_SIZE;
    // 0, 8, 0, 8,...0, 8       warp 0
    // 16, 24, 16, 24,...16, 24 warp 1
    // 32, 40, 32, 40,...32, 40 warp 2
    // 48, 56, 48, 56,...48, 56 warp 3
    const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset;
    // shared one block table ?
    const int block_index = (token_idx >= context_len) ? 0 : token_idx;
    const int physical_block_number = block_table[block_index];
    // different qk, l_vec one thread take a vector
    L_vec logits_vec;
    from_float(logits_vec, *reinterpret_cast<Float_L_vec*>(logits + token_idx));

    const scalar_t* v_ptr = v_cache + physical_block_number * num_heads * HEAD_SIZE * BLOCK_SIZE
                                    + head_idx * HEAD_SIZE * BLOCK_SIZE;
#pragma unroll // 8 loops 
    for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
      // lane / 2 + i * 16
      // 0, 0, ..., 15, 15 ... 0, 0, ..., 15, 15 ... loop 0
      // 16, 16, ..., 31, 31 ... 16, 16, ..., 31, 31 ... loop 1
      // ...
      // 112, 112, ..., 127, 127 ... 112, 112, ..., 127, 127 ... loop 7
      const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
      if (row_idx < HEAD_SIZE) {
        // 0, 8, 16, 24, ... 15*16, 15*16+8 ...  loop 0  
        // 16*16, 16*16+8, 17*16, 17*16+8, ... 31*16, 31*16+8 ...  loop 1
        // ...
        // 112*16, 112*16+8, 113*16, 113*16+8, ... 127*16, 127*16+8 ...  loop 7
        const int offset = row_idx * BLOCK_SIZE + physical_block_offset;
        V_vec v_vec = *reinterpret_cast<const V_vec*>(v_ptr + offset);
        // v_cache [num_blocks, num_heads, head_size, block_size]
        // q*k == 1*head_size x head_size*num_tokens == 1*num_tokens
        // l*v == 1*num_tokens x num_tokens*head_size == 1*head_size
        //   0 .... 16
        // 0
        // .
        // .
        // .
        // 128
        // vec size is 8, warp loop time = 128 * 16 / (8*32) = 8 
        // due cross token warp, so need use add
        // 0, .. 15, 16, ... , 31,  32, ..., 47, 48, ... ,63  block_idx loop 0
        // 64, ..,79                                          block_idx loop 1
        accs[i] += dot(logits_vec, v_vec);
      }
    }
  }

  // Perform reduction within each warp.
#pragma unroll
  for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
    float acc = accs[i];
#pragma unroll // do row reduce, due to once calc 8 number dot, we have 16 tokens in one row
    for (int mask = NUM_V_VECS_PER_ROW / 2; mask >= 1; mask /= 2) {
      acc += __shfl_xor_sync(uint32_t(-1), acc, mask);
    }
    accs[i] = acc;
  }

  // NOTE(woosuk): A barrier is required because the shared memory space for logits
  // is reused for the output.
  __syncthreads();

  // Perform reduction across warps.
  float* out_smem = reinterpret_cast<float*>(shared_mem);
#pragma unroll
  for (int i = NUM_WARPS; i > 1; i /= 2) { // NUM_WARPS 4
    int mid = i / 2; // 2, 1 loop 2 times, separately reduce warp 2,3, and warp 1
    // Upper warps write to shared memory.
    if (warp_idx >= mid && warp_idx < i) { // warp 2, 3 selected
      // (warp_idx - mid) * HEAD_SIZE -- > 0*128, 1*128
      float* dst = &out_smem[(warp_idx - mid) * HEAD_SIZE];
#pragma unroll
      for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) { // 8 loops
        // (0, 1, 2, ..., 31) /2  + i * 16
        // 0, 0, ..., 15, 15       loop 0
        // 16, 16, ..., 31, 31     loop 1
        // ...
        // 112, 112, ..., 127, 127 loop 7
        const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
        // select thread group 0
        // 0, 1, 2, ... , 127  --row_idx
        // 0, 1, 2, ... , 127  --token idx
        if (row_idx < HEAD_SIZE && lane % NUM_V_VECS_PER_ROW == 0) {
          dst[row_idx] = accs[i];
        }
      }
    }
    __syncthreads();

    // Lower warps update the output.
    // cross warp reduce
    // add warp 2, 3 to warp 0, 1
    if (warp_idx < mid) {
      const float* src = &out_smem[warp_idx * HEAD_SIZE];
#pragma unroll
      for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
        const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
        if (row_idx < HEAD_SIZE && lane % NUM_V_VECS_PER_ROW == 0) {
          accs[i] += src[row_idx];
        }
      }
    }
    __syncthreads();
  }

  // Write the final output.
  if (warp_idx == 0) {
    scalar_t* out_ptr = out + seq_idx * num_heads * HEAD_SIZE + head_idx * HEAD_SIZE;
#pragma unroll
    for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
      const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
      if (row_idx < HEAD_SIZE && lane % NUM_V_VECS_PER_ROW == 0) {
        from_float(*(out_ptr + row_idx), accs[i]);
      }
    }
  }
}

} // namespace vllm


#define LAUNCH_XL_ATTENTION_KERNEL(T, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS)                        \
  vllm::xl_single_query_cached_kv_attention_kernel<T, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS>        \
  <<<grid, block, shared_mem_size, stream>>>(                                                 \
    out_ptr,                                                                                  \
    query_ptr,                                                                                \
    key_cache_ptr,                                                                            \
    value_cache_ptr,                                                                          \
    pos_bias_u_ptr,                                                                           \
    matrix_bd_ptr,                                                                            \
    scale,                                                                                    \
    block_tables_ptr,                                                                         \
    context_lens_ptr,                                                                         \
    max_num_blocks_per_seq,                                                                   \
    query_stride);


// TODO(woosuk): Tune NUM_THREADS.
template<
  typename T,
  int BLOCK_SIZE,
  int NUM_THREADS = 128>
void xl_single_query_cached_kv_attention_launcher(
  torch::Tensor& out,
  torch::Tensor& query,
  torch::Tensor& key_cache,
  torch::Tensor& value_cache,
  torch::Tensor& pos_bias_u,
  torch::Tensor& matrix_bd,
  float scale,
  torch::Tensor& block_tables,
  torch::Tensor& context_lens,
  int max_context_len) {
  int num_seqs = query.size(0);
  int num_heads = query.size(1);
  int head_size = query.size(2);
  int max_num_blocks_per_seq = block_tables.size(1);
  int query_stride = query.stride(0);

  int thread_group_size = MAX(WARP_SIZE / BLOCK_SIZE, 1);
  assert(head_size % thread_group_size == 0);

  T* out_ptr = reinterpret_cast<T*>(out.data_ptr());
  T* query_ptr = reinterpret_cast<T*>(query.data_ptr());
  T* key_cache_ptr = reinterpret_cast<T*>(key_cache.data_ptr());
  T* value_cache_ptr = reinterpret_cast<T*>(value_cache.data_ptr());
  T* pos_bias_u_ptr = reinterpret_cast<T*>(pos_bias_u.data_ptr());
  T* matrix_bd_ptr = reinterpret_cast<T*>(matrix_bd.data_ptr());
  int* block_tables_ptr = block_tables.data_ptr<int>();
  int* context_lens_ptr = context_lens.data_ptr<int>();

  constexpr int NUM_WARPS = NUM_THREADS / WARP_SIZE;
  int padded_max_context_len = ((max_context_len + BLOCK_SIZE - 1) / BLOCK_SIZE) * BLOCK_SIZE;
  int logits_size = padded_max_context_len * sizeof(float);
  int outputs_size = (NUM_WARPS / 2) * head_size * sizeof(float);
  int shared_mem_size = std::max(logits_size, outputs_size);

  dim3 grid(num_heads, num_seqs);
  dim3 block(NUM_THREADS);
  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
  switch (head_size) {
    // NOTE(woosuk): To reduce the compilation time, we omitted head sizes
    // 32, 160, 192, 256.
    // case 32:
    //   LAUNCH_ATTENTION_KERNEL(T, 32, BLOCK_SIZE, NUM_THREADS);
    //   break;
    case 64:
      LAUNCH_XL_ATTENTION_KERNEL(T, 64, BLOCK_SIZE, NUM_THREADS);
      break;
    case 80:
      LAUNCH_XL_ATTENTION_KERNEL(T, 80, BLOCK_SIZE, NUM_THREADS);
      break;
    case 96:
      LAUNCH_XL_ATTENTION_KERNEL(T, 96, BLOCK_SIZE, NUM_THREADS);
      break;
    case 128:
      LAUNCH_XL_ATTENTION_KERNEL(T, 128, BLOCK_SIZE, NUM_THREADS);
      break;
    // case 160:
    //   LAUNCH_ATTENTION_KERNEL(T, 160, BLOCK_SIZE, NUM_THREADS);
    //   break;
    // case 192:
    //   LAUNCH_ATTENTION_KERNEL(T, 192, BLOCK_SIZE, NUM_THREADS);
    //   break;
    // case 256:
    //   LAUNCH_ATTENTION_KERNEL(T, 256, BLOCK_SIZE, NUM_THREADS);
    //   break;
    default:
      TORCH_CHECK(false, "Unsupported head size: ", head_size);
      break;
  }
}

#define CALL_XL_KERNEL_LAUNCHER(T, BLOCK_SIZE)                         \
  xl_single_query_cached_kv_attention_launcher<T, BLOCK_SIZE>(         \
    out,                                                            \
    query,                                                          \
    key_cache,                                                      \
    value_cache,                                                    \
    pos_bias_u,                                                     \
    matrix_bd,                                                      \
    scale,                                                          \
    block_tables,                                                   \
    context_lens,                                                   \
    max_context_len);

// NOTE(woosuk): To reduce the compilation time, we omitted block sizes
// 1, 2, 4, 64, 128, 256.
#define CALL_XL_KERNEL_LAUNCHER_BLOCK_SIZE(T)                          \
  switch (block_size) {                                             \
    /* case 1:                         */                           \
    /*   CALL_KERNEL_LAUNCHER(T, 1);   */                           \
    /*   break;                        */                           \
    /* case 2:                         */                           \
    /*   CALL_KERNEL_LAUNCHER(T, 2);   */                           \
    /*   break;                        */                           \
    /* case 4:                         */                           \
    /*   CALL_KERNEL_LAUNCHER(T, 4);   */                           \
    /*   break;                        */                           \
    case 8:                                                         \
      CALL_XL_KERNEL_LAUNCHER(T, 8);                                   \
      break;                                                        \
    case 16:                                                        \
      CALL_XL_KERNEL_LAUNCHER(T, 16);                                  \
      break;                                                        \
    case 32:                                                        \
      CALL_XL_KERNEL_LAUNCHER(T, 32);                                  \
      break;                                                        \
    /* case 64:                        */                           \
    /*   CALL_KERNEL_LAUNCHER(T, 64);  */                           \
    /*   break;                        */                           \
    /* case 128:                       */                           \
    /*   CALL_KERNEL_LAUNCHER(T, 128); */                           \
    /*   break;                        */                           \
    /* case 256:                       */                           \
    /*   CALL_KERNEL_LAUNCHER(T, 256); */                           \
    /*   break;                        */                           \
    default:                                                        \
      TORCH_CHECK(false, "Unsupported block size: ", block_size);   \
      break;                                                        \
  }

void xl_single_query_cached_kv_attention(
  torch::Tensor& out,             // [num_seqs, num_heads, head_size]
  torch::Tensor& query,           // [num_seqs, num_heads, head_size]
  torch::Tensor& key_cache,       // [num_blocks, num_heads, head_size/x, block_size, x]
  torch::Tensor& value_cache,     // [num_blocks, num_heads, head_size, block_size]
  torch::Tensor& pos_bias_u,      // [num_seqs, num_heads, head_size]
  torch::Tensor& matrix_bd,       // [num_seqs, num_heads, head_size]
  float scale,
  torch::Tensor& block_tables,    // [num_seqs, max_num_blocks_per_seq]
  torch::Tensor& context_lens,    // [num_seqs]
  int block_size,
  int max_context_len) {
  if (query.dtype() == at::ScalarType::Float) {
    CALL_XL_KERNEL_LAUNCHER_BLOCK_SIZE(float);
  } else if (query.dtype() == at::ScalarType::Half) {
    CALL_XL_KERNEL_LAUNCHER_BLOCK_SIZE(__half);
  } else if (query.dtype() == at::ScalarType::BFloat16) {
    CALL_XL_KERNEL_LAUNCHER_BLOCK_SIZE(__nv_bfloat16);
  } else {
    TORCH_CHECK(false, "Unsupported data type: ", query.dtype());
  }
}






#undef WARP_SIZE
#undef MAX
#undef MIN
